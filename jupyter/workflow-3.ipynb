{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKFLOW 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "theme_style = \"darkgrid\"\n",
    "color_palette_style = \"deep\"\n",
    "palette = sns.color_palette(color_palette_style)\n",
    "\n",
    "# Set a warm-up phase (e.g., ignore the first 10 executions)\n",
    "warmup_executions = 100  # Adjust as needed\n",
    "\n",
    "days = [\n",
    "    \"2025-02-03\", \"2025-02-04\", \"2025-02-05\", \"2025-02-06\", \"2025-02-07\", \"2025-02-08\", \"2025-02-09\"\n",
    "]\n",
    "day_labels = [\n",
    "    \"Day 1\", \"Day 2\", \"Day 3\", \"Day 4\", \"Day 5\", \"Day 6\", \"Day 7\"\n",
    "]\n",
    "\n",
    "window_size = 200\n",
    "\n",
    "folder_break_even = \"./experiment_plots/break-even-analysis\"\n",
    "file_ending = \".pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup directories\n",
    "os.makedirs(folder_break_even, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format minutes to time\n",
    "def format_minutes_to_time(minutes):\n",
    "    \"\"\"Convert decimal minutes to MM:SS format.\"\"\"\n",
    "    total_seconds = int(minutes * 60)\n",
    "    mm = total_seconds // 60\n",
    "    ss = total_seconds % 60\n",
    "    return f\"{mm:02}:{ss:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate break even\n",
    "def calculate_break_even_day(baseline_df, optimized_df, warmup_executions=0):\n",
    "    # Sort and prepare data\n",
    "    baseline_df = baseline_df.sort_values(by=\"timestamp\").reset_index(drop=True)\n",
    "    optimized_df = optimized_df.sort_values(by=\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # Convert timestamp to datetime\n",
    "    baseline_df[\"timestamp\"] = pd.to_datetime(baseline_df[\"timestamp\"])\n",
    "    optimized_df[\"timestamp\"] = pd.to_datetime(optimized_df[\"timestamp\"])\n",
    "\n",
    "    # Compute cumulative execution time\n",
    "    baseline_df[\"cumulative_time\"] = baseline_df[\"execution_time\"].cumsum()\n",
    "    optimized_df[\"cumulative_time\"] = optimized_df[\"execution_time\"].cumsum()\n",
    "\n",
    "    # Compute elapsed time in minutes\n",
    "    baseline_df[\"duration_minutes\"] = (baseline_df[\"timestamp\"] - baseline_df[\"timestamp\"].min()).dt.total_seconds() / 60\n",
    "    optimized_df[\"duration_minutes\"] = (optimized_df[\"timestamp\"] - optimized_df[\"timestamp\"].min()).dt.total_seconds() / 60\n",
    "\n",
    "    # Trim the optimized function to match the length of the baseline function\n",
    "    optimized_df_trimmed = optimized_df.iloc[:len(baseline_df)].reset_index(drop=True)\n",
    "\n",
    "    # Find break-even point (ignore first N executions)\n",
    "    break_even_index = (\n",
    "        (optimized_df_trimmed[\"cumulative_time\"] < baseline_df[\"cumulative_time\"])\n",
    "        & (optimized_df_trimmed.index >= warmup_executions)\n",
    "    ).idxmax()\n",
    "\n",
    "    return baseline_df, optimized_df, break_even_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate break even each day\n",
    "def calculate_break_even_each_day():\n",
    "    optimized_df_list = []\n",
    "    baseline_df_list = []\n",
    "    # List to hold break-even points for each day\n",
    "    break_even_points = []\n",
    "    break_even_indices = []\n",
    "\n",
    "    # Loop through each day and calculate the break-even point\n",
    "    for day in days:\n",
    "        # Load the data for the current day\n",
    "        baseline_df = pd.read_csv(f\"./logs_analysis/{day}/execution_1/{day}-baselineFunction-logs.csv\")\n",
    "        optimized_df = pd.read_csv(f\"./logs_analysis/{day}/execution_1/{day}-optimizedFunction-logs.csv\")\n",
    "        \n",
    "        # Get the break-even point for this day\n",
    "        baseline_df, optimized_df, break_even_index = calculate_break_even_day(baseline_df, optimized_df, warmup_executions)\n",
    "\n",
    "        # Store the dataframes for later use\n",
    "        baseline_df_list.append(baseline_df)\n",
    "        optimized_df_list.append(optimized_df)\n",
    "\n",
    "        # Calculate the break-even time and append it to the list\n",
    "        break_even_time = baseline_df[\"duration_minutes\"].iloc[break_even_index]\n",
    "        break_even_points.append(break_even_time)\n",
    "        break_even_indices.append(break_even_index)\n",
    "\n",
    "    return optimized_df_list, baseline_df_list, break_even_points, break_even_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot break even\n",
    "def break_even_plot(baseline_df, optimized_df, break_even_index, day_index):\n",
    "    # Create a Seaborn plot\n",
    "    sns.set_style(theme_style)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "\n",
    "    # Plot cumulative execution times with elapsed minutes\n",
    "    sns.lineplot(x=optimized_df[\"duration_minutes\"], y=optimized_df[\"cumulative_time\"], label=\"Optimized Function\", linestyle=\"-\", color=palette[0])\n",
    "    sns.lineplot(x=baseline_df[\"duration_minutes\"], y=baseline_df[\"cumulative_time\"], label=\"Baseline Function\", linestyle=\"-\", color=palette[1])\n",
    "\n",
    "    # Mark the break-even point\n",
    "    bep_time = baseline_df[\"duration_minutes\"].iloc[break_even_index]\n",
    "    break_even_str = format_minutes_to_time(bep_time)\n",
    "    plt.axvline(bep_time, color=\"red\", linestyle=\"dotted\", label=f\"Break-even at {break_even_str} min\")\n",
    "    \n",
    "    # plt.yscale(\"log\")\n",
    "\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlim(right=30)\n",
    "    plt.ylim(bottom=0)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Duration (min)\")\n",
    "    plt.ylabel(\"Cumulative Execution Time (ms)\")\n",
    "    plt.title(f\"Break-Even Analysis: {day_labels[day_index]}\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # SAVE PLOT\n",
    "    plot_filename = os.path.join(folder_break_even, f\"{days[day_index]}-break-even-analysis\" + file_ending)\n",
    "    plt.savefig(plot_filename)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot break even grouped\n",
    "def break_even_plot_grouped(df_all_days_optimized, df_all_days_baseline, optimized_df_list, baseline_df_list, break_even_points, avg_break_even, median_break_even, break_even_indices):\n",
    "    # Create a Seaborn plot for all the days combined\n",
    "    sns.set_style(theme_style)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    sns.lineplot(x=df_all_days_optimized[\"duration_minutes\"], y=df_all_days_optimized[\"cumulative_time\"], label=f\"Optimized Function (median)\", linestyle=\"-\", color=palette[0])\n",
    "    sns.lineplot(x=df_all_days_baseline[\"duration_minutes\"], y=df_all_days_baseline[\"cumulative_time\"], label=f\"Baseline Function (median)\", linestyle=\"-\", color=palette[1])\n",
    "    \n",
    "    # Plot each day's cumulative execution times with elapsed minutes\n",
    "    for idx, (baseline_df, optimized_df) in enumerate(zip(baseline_df_list, optimized_df_list)):    \n",
    "        # Mark the break-even point for each day\n",
    "        bep_time = baseline_df[\"duration_minutes\"].iloc[baseline_df[\"duration_minutes\"].searchsorted(break_even_points[idx])]\n",
    "        plt.axvline(bep_time, color=\"gray\", linestyle=\"dotted\", alpha=0.5)\n",
    "\n",
    "    # Convert times to MM:SS format\n",
    "    avg_break_even_str = format_minutes_to_time(avg_break_even)\n",
    "    median_break_even_str = format_minutes_to_time(median_break_even)\n",
    "\n",
    "    # Add vertical lines for the average and median break-even times\n",
    "    plt.axvline(avg_break_even, color=palette[3], linestyle=\"--\", label=f\"Average Break-even: {avg_break_even_str} min\")\n",
    "    plt.axvline(median_break_even, color=palette[4], linestyle=\"--\", label=f\"Median Break-even: {median_break_even_str} min\")\n",
    "\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlim(right=30)\n",
    "    plt.ylim(bottom=0)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Duration (min)\")\n",
    "    plt.ylabel(\"Cumulative Execution Time (ms)\")\n",
    "    plt.title(f\"Break-Even Analysis: All Days\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # SAVE PLOT\n",
    "    plot_filename = os.path.join(folder_break_even, \"grouped-break-even-analysis\" + file_ending)\n",
    "    plt.savefig(plot_filename)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather grouped data\n",
    "def gather_optimized_all_days():\n",
    "    # prepare optimized data\n",
    "    df_list = []\n",
    "    for date in days:\n",
    "        file_path = f'./logs_analysis/{date}/execution_1/{date}-optimizedFunction-logs.csv'\n",
    "        if os.path.exists(file_path):  # Check if file exists to avoid errors\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])  # Convert timestamp to datetime\n",
    "            df['duration_minutes'] = (df['timestamp'] - df['timestamp'].min()).dt.total_seconds() / 60  # Normalize time\n",
    "            df['smoothed_execution_time'] = df['execution_time'].rolling(window=window_size).mean()\n",
    "\n",
    "            df_list.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    df_optimized_all_days = pd.concat(df_list, ignore_index=True)\n",
    "    # Sort the dataframe by duration_minutes to ensure proper rolling calculations\n",
    "    df_optimized_all_days = df_optimized_all_days.sort_values(by=['duration_minutes', 'duration_minutes'])\n",
    "    # Compute rolling median & std for optimized function\n",
    "    df_optimized_all_days['smoothed_median'] = df_optimized_all_days['execution_time'].rolling(window=window_size, center=True).median()\n",
    "    # Drop NaNs\n",
    "    df_optimized_all_days = df_optimized_all_days.dropna(subset=['smoothed_median'])\n",
    "    return df_optimized_all_days\n",
    "\n",
    "def gather_baseline_all_days():\n",
    "    # prepare optimized data\n",
    "    df_list = []\n",
    "    for date in days:\n",
    "        file_path = f'./logs_analysis/{date}/execution_1/{date}-baselineFunction-logs.csv'\n",
    "        if os.path.exists(file_path):  # Check if file exists to avoid errors\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])  # Convert timestamp to datetime\n",
    "            df['duration_minutes'] = (df['timestamp'] - df['timestamp'].min()).dt.total_seconds() / 60  # Normalize time\n",
    "            df['smoothed_execution_time'] = df['execution_time'].rolling(window=window_size).mean()\n",
    "\n",
    "            df_list.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    df_baseline_all_days = pd.concat(df_list, ignore_index=True)\n",
    "    # Sort the dataframe by duration_minutes to ensure proper rolling calculations\n",
    "    df_baseline_all_days = df_baseline_all_days.sort_values(by=['duration_minutes', 'duration_minutes'])\n",
    "    # Compute rolling median & std for optimized function\n",
    "    df_baseline_all_days['smoothed_median'] = df_baseline_all_days['execution_time'].rolling(window=window_size, center=True).median()\n",
    "    # Drop NaNs\n",
    "    df_baseline_all_days = df_baseline_all_days.dropna(subset=['smoothed_median'])\n",
    "    return df_baseline_all_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ALL DAYS INDIVIDUALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute break even for all 7 days\n",
    "break_even_points = []\n",
    "\n",
    "# Loop through each day and calculate the break-even point\n",
    "for idx, day in enumerate(days):\n",
    "    # Load the data for the current day\n",
    "    baseline_df = pd.read_csv(f\"./logs_analysis/{day}/execution_1/{day}-baselineFunction-logs.csv\")\n",
    "    optimized_df = pd.read_csv(f\"./logs_analysis/{day}/execution_1/{day}-optimizedFunction-logs.csv\")\n",
    "\n",
    "    # Get the break-even point for this day\n",
    "    baseline_df, optimized_df, break_even_index = calculate_break_even_day(baseline_df, optimized_df, warmup_executions)\n",
    "\n",
    "    # Calculate the break-even time\n",
    "    break_even_time = baseline_df[\"duration_minutes\"].iloc[break_even_index]\n",
    "    break_even_points.append(break_even_time)\n",
    "\n",
    "    break_even_plot(baseline_df, optimized_df, break_even_index, idx)\n",
    "\n",
    "    # Calculate the break-even time\n",
    "    break_even_time = baseline_df[\"duration_minutes\"].iloc[break_even_index]\n",
    "    break_even_points.append(break_even_time)\n",
    "\n",
    "    # print(\"Optimized Table (First Rows):\")\n",
    "    # print(optimized_df[[\"execution_time\", \"cumulative_time\"]].head())\n",
    "\n",
    "    # print(\"\\nBaseline Table (First Rows):\")\n",
    "    # print(baseline_df[[\"execution_time\", \"cumulative_time\"]].head())\n",
    "\n",
    "    # print(\"Optimized Table (Last Rows):\")\n",
    "    # print(optimized_df[[\"execution_time\", \"cumulative_time\"]].tail())\n",
    "\n",
    "    # print(\"\\nBaseline Table (Last Rows):\")\n",
    "    # print(baseline_df[[\"execution_time\", \"cumulative_time\"]].tail())\n",
    "\n",
    "\n",
    "# Calculate the average or median break-even point across all days\n",
    "avg_break_even = sum(break_even_points) / len(break_even_points)\n",
    "median_break_even = pd.Series(break_even_points).median()\n",
    "\n",
    "print(f\"Average break-even time: {avg_break_even:.2f} min\")\n",
    "print(f\"Median break-even time: {median_break_even:.2f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ALL DAYS GROUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPED FIGURE\n",
    "all_optimized_df = gather_optimized_all_days()\n",
    "all_baseline_df = gather_baseline_all_days()\n",
    "\n",
    "# Compute cumulative execution time\n",
    "# all_optimized_df[\"cumulative_time\"] = all_optimized_df[\"smoothed_median\"].cumsum()\n",
    "# all_baseline_df[\"cumulative_time\"] = all_baseline_df[\"smoothed_median\"].cumsum()\n",
    "all_optimized_df[\"cumulative_time\"] = all_optimized_df[\"execution_time\"].cumsum()\n",
    "all_baseline_df[\"cumulative_time\"] = all_baseline_df[\"execution_time\"].cumsum()\n",
    "\n",
    "# Calculate break evens for each day\n",
    "optimized_df_list, baseline_df_list, break_even_points, break_even_indices = calculate_break_even_each_day()\n",
    "# Calculate the average and median break-even times\n",
    "avg_break_even = sum(break_even_points) / len(break_even_points)\n",
    "median_break_even = pd.Series(break_even_points).median()\n",
    "\n",
    "# Plot the break-even analysis for all days combined\n",
    "break_even_plot_grouped(all_optimized_df, all_baseline_df, optimized_df_list, baseline_df_list, break_even_points, avg_break_even, median_break_even, break_even_indices)\n",
    "\n",
    "print(sorted(break_even_points))\n",
    "print(sorted(break_even_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARCHIVE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvJupyterBSC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
